{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce9302f-1357-49c6-99c1-b63f62d8a2e3",
   "metadata": {},
   "source": [
    "# 🧪 Scraping Test - Pipeline de scraping (étape 1)\n",
    "\n",
    "Ce notebook a pour objectif de tester le pipeline de scraping sur un échantillon de produits depuis eBay.\n",
    "\n",
    "🎯 **Objectif** :\n",
    "- Identifier les balises HTML utiles\n",
    "- Tester la récupération des informations (titre, prix, lien)\n",
    "- Éviter la surcharge mémoire, disque et processeur\n",
    "- Valider la faisabilité du scraping\n",
    "\n",
    "🔄 **Pipeline de scraping (résumé)** :\n",
    "1. Définir les sites à scraper\n",
    "2. Inspecter la structure HTML des pages\n",
    "3. Identifier les sélecteurs (balises CSS/HTML)\n",
    "4. Automatiser avec `requests` et `BeautifulSoup`\n",
    "5. Limiter à quelques résultats (`limit=5`)\n",
    "6. Ne pas stocker localement pour ce test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd673f8e-a0a0-4d7d-b5ff-be908ad81735",
   "metadata": {},
   "source": [
    "## IMPORTAION BIBLIOTHEQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae3eb36-4db3-43fd-b019-4c3113e1249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Installer toutes les bibliothèques nécessaires\n",
    "#!pip install tweepy pandas beautifulsoup4 requests lxml snscrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56458748-858b-4be2-b63e-a83e1a1fc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import re\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0f553-cc61-4a0c-9a4c-7804bcbda48d",
   "metadata": {},
   "source": [
    "## TWITTER SCRAPIING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd1fff5-b658-454a-812d-44d1a9fd6b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token chargé : True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "BEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\n",
    "\n",
    "print(\"Token chargé :\", BEARER_TOKEN is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ebc4da0-f356-4a71-a073-14bf565b53cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequests",
     "evalue": "429 Too Many Requests\nToo Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTooManyRequests\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m client = tweepy.Client(bearer_token=BEARER_TOKEN)\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33m(#beauty OR #fashion OR #tech OR #food) -is:retweet lang:en\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tweets = client.search_recent_tweets(query=query, max_results=\u001b[32m5\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets.data:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(tweet.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\produits_env\\Lib\\site-packages\\tweepy\\client.py:1270\u001b[39m, in \u001b[36mClient.search_recent_tweets\u001b[39m\u001b[34m(self, query, user_auth, **params)\u001b[39m\n\u001b[32m   1178\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"search_recent_tweets( \\\u001b[39;00m\n\u001b[32m   1179\u001b[39m \u001b[33;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[32m   1180\u001b[39m \u001b[33;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1267\u001b[39m \u001b[33;03m.. _Academic Research Project: https://developer.twitter.com/en/docs/projects\u001b[39;00m\n\u001b[32m   1268\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1269\u001b[39m params[\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m] = query\n\u001b[32m-> \u001b[39m\u001b[32m1270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_request(\n\u001b[32m   1271\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m/2/tweets/search/recent\u001b[39m\u001b[33m\"\u001b[39m, params=params,\n\u001b[32m   1272\u001b[39m     endpoint_parameters=(\n\u001b[32m   1273\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mend_time\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexpansions\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmax_results\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmedia.fields\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1274\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnext_token\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mplace.fields\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpoll.fields\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1275\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msince_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msort_order\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstart_time\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtweet.fields\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1276\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33muntil_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33muser.fields\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1277\u001b[39m     ), data_type=Tweet, user_auth=user_auth\n\u001b[32m   1278\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\produits_env\\Lib\\site-packages\\tweepy\\client.py:129\u001b[39m, in \u001b[36mBaseClient._make_request\u001b[39m\u001b[34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_request\u001b[39m(\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m, method, route, params={}, endpoint_parameters=(), json=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    125\u001b[39m     data_type=\u001b[38;5;28;01mNone\u001b[39;00m, user_auth=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    126\u001b[39m ):\n\u001b[32m    127\u001b[39m     request_params = \u001b[38;5;28mself\u001b[39m._process_params(params, endpoint_parameters)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.request(method, route, params=request_params,\n\u001b[32m    130\u001b[39m                             json=json, user_auth=user_auth)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_type \u001b[38;5;129;01mis\u001b[39;00m requests.Response:\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\produits_env\\Lib\\site-packages\\tweepy\\client.py:115\u001b[39m, in \u001b[36mBaseClient.request\u001b[39m\u001b[34m(self, method, route, params, json, user_auth)\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(method, route, params, json, user_auth)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(response)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code >= \u001b[32m500\u001b[39m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TwitterServerError(response)\n",
      "\u001b[31mTooManyRequests\u001b[39m: 429 Too Many Requests\nToo Many Requests"
     ]
    }
   ],
   "source": [
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "query = \"(#beauty OR #fashion OR #tech OR #food) -is:retweet lang:en\"\n",
    "tweets = client.search_recent_tweets(query=query, max_results=5)\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(tweet.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529146a8-dfd0-47f2-88e4-2b05432e60f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\produits_env\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet ID: 1926399545769963664\n",
      "Date: 2025-05-24 22:06:57+00:00\n",
      "Texte: “𝑩𝒆𝒂𝒖𝒕𝒚 𝒘𝒊𝒍𝒍 𝒔𝒂𝒗𝒆 𝒕𝒉𝒆 𝒘𝒐𝒓𝒍𝒅.” ~𝑭𝒚𝒐𝒅𝒐𝒓 𝑫𝒐𝒔𝒕𝒐𝒆𝒗𝒔𝒌𝒚 🧡 #art #beauty #faith #hope #love #inspiration #thoughts #journal #diary #nature\n",
      "https://t.co/2PeV5n0YvM\n",
      "Likes: 1, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926399530565603820\n",
      "Date: 2025-05-24 22:06:53+00:00\n",
      "Texte: #Beach #Breeze #Beauty\n",
      "\n",
      "#merch with #digitaltease #digitalart now on your favourite #products via @redbubble\n",
      "👇\n",
      "https://t.co/qgBVuYC8Qz\n",
      "\n",
      "#digitalart #shop #printondemand #findyourthing #giftideas #redbubble #shopfromhome #retailtherapy #onlineshopping #presents #gift https://t.co/tn6KIPJ3EA\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926399478430322969\n",
      "Date: 2025-05-24 22:06:41+00:00\n",
      "Texte: Need a #tee that says “Everything will be ok coz I’m here”? Ready to rock your #style with a dash of confidence? Who’s in for some comfy #vibes? #fashion #hikerunner\n",
      "https://t.co/DsApBV9htI https://t.co/VWe1CmMsYR\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926399168303493417\n",
      "Date: 2025-05-24 22:05:27+00:00\n",
      "Texte: Underwater PC Edition \n",
      " https://t.co/uZseZ5Qvq4\n",
      "#tech\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926399101677015544\n",
      "Date: 2025-05-24 22:05:11+00:00\n",
      "Texte: @TemptingFoodNow #Food 🎉👨‍🍳🎖️ Silky, Savory Chinese Rice Porridge (Congee) https://t.co/F7nTKJ85SN\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926399094429200872\n",
      "Date: 2025-05-24 22:05:09+00:00\n",
      "Texte: Chat with Vera: The Traveling Taco by Mia Wenjen [Review &amp; Giveaway] https://t.co/ZjMeeLsdcL via Chat With Vera #ReadYourWorld #picturebook #giveaway #food #immigrants @redcometpress https://t.co/M6dA7KMFcb\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926399079220760629\n",
      "Date: 2025-05-24 22:05:05+00:00\n",
      "Texte: So beautiful 😘🥰#instagram   #facebookreels  #reelsviralシ #reelsvideoシ #fashion #facebookpost #facebookviral #facebookvideo \n",
      "#trending  #my_chuuu #KIA #KIATigers  #facebookviral  #photography #kpop #이주은 #LeeJuEun  #cheerleader #cute #beautiful #pretty  #dance #sports #bea https://t.co/I4sEFLvh5F\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926399036191371472\n",
      "Date: 2025-05-24 22:04:55+00:00\n",
      "Texte: how to\n",
      "\n",
      "#fua #costumejewelry  #fashion #accessory\n",
      "\n",
      "https://t.co/OBJGPU57BJ\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926398987449368971\n",
      "Date: 2025-05-24 22:04:44+00:00\n",
      "Texte: life is good all the time ☀️☀️☀️\n",
      "\n",
      "#barcelona #fashion #Lifestyle https://t.co/bIxoGWkeiU\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n",
      "Tweet ID: 1926398660855693520\n",
      "Date: 2025-05-24 22:03:26+00:00\n",
      "Texte: @PCshepherd24 Pats fan but Daboll on here is kinda ridiculous. Consistently has had a roster full of dustballs and got a playoff win with Daniel Jones #JustADrunkMansUnbiasedOpinion #Tech\n",
      "Likes: 0, Retweets: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "# Tes clés d'API Twitter (copie/colle les tiennes)\n",
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAALfq1wEAAAAAGJFkMSLlYdltHwckflUqwWLooGk%3DHlcJoKFxuftBXvCvVL1pH0bh3EBj3VlugSBZYzZV3dYrOxXacw\"\n",
    "\n",
    "# Connexion à l'API avec Tweepy v2\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Exemple simple : rechercher les tweets récents contenant certains hashtags dans 4 catégories (beauty, fashion, tech, food)\n",
    "\n",
    "query = \"(#beauty OR #fashion OR #tech OR #food) -is:retweet lang:en\"\n",
    "\n",
    "# Récupérer les 10 tweets récents\n",
    "tweets = client.search_recent_tweets(query=query, max_results=10, tweet_fields=['public_metrics', 'created_at'])\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(f\"Tweet ID: {tweet.id}\")\n",
    "    print(f\"Date: {tweet.created_at}\")\n",
    "    print(f\"Texte: {tweet.text}\")\n",
    "    print(f\"Likes: {tweet.public_metrics['like_count']}, Retweets: {tweet.public_metrics['retweet_count']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f5a950-f2fc-406d-a105-6df9854f2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\produits_env\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur inattendue : 401 Unauthorized\n",
      "Unauthorized\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Charger les tokens depuis le .env (deux tokens ici)\n",
    "BEARER_TOKENS = [\n",
    "    os.getenv(\"BEARER_TOKEN_1\"),\n",
    "    os.getenv(\"BEARER_TOKEN_2\")\n",
    "]\n",
    "\n",
    "def get_client(token):\n",
    "    return tweepy.Client(bearer_token=token)\n",
    "\n",
    "def search_tweets_with_pagination(tokens, query, max_results=10, max_pages=5):\n",
    "    tweets_data = []\n",
    "    next_token = None\n",
    "    pages_fetched = 0\n",
    "    token_index = 0\n",
    "    client = get_client(tokens[token_index])\n",
    "    \n",
    "    while pages_fetched < max_pages:\n",
    "        try:\n",
    "            response = client.search_recent_tweets(\n",
    "                query=query,\n",
    "                max_results=max_results,\n",
    "                tweet_fields=['public_metrics', 'created_at', 'lang'],\n",
    "                next_token=next_token\n",
    "            )\n",
    "            if not response.data:\n",
    "                print(\"Plus de tweets disponibles.\")\n",
    "                break\n",
    "            \n",
    "            for tweet in response.data:\n",
    "                tweets_data.append({\n",
    "                    \"id\": tweet.id,\n",
    "                    \"date\": tweet.created_at,\n",
    "                    \"texte\": tweet.text,\n",
    "                    \"likes\": tweet.public_metrics['like_count'],\n",
    "                    \"retweets\": tweet.public_metrics['retweet_count'],\n",
    "                    \"langue\": tweet.lang\n",
    "                })\n",
    "            \n",
    "            pages_fetched += 1\n",
    "            print(f\"Page {pages_fetched} récupérée, total tweets: {len(tweets_data)}\")\n",
    "            \n",
    "            next_token = response.meta.get('next_token', None)\n",
    "            if not next_token:\n",
    "                print(\"Fin de la pagination.\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        except tweepy.TooManyRequests:\n",
    "            print(f\"Limite API atteinte pour token {token_index + 1}, changement de token...\")\n",
    "            token_index += 1\n",
    "            if token_index >= len(tokens):\n",
    "                print(\"Tous les tokens sont épuisés, pause de 15 minutes...\")\n",
    "                time.sleep(15 * 60)\n",
    "                token_index = 0\n",
    "            client = get_client(tokens[token_index])\n",
    "            next_token = None  # Recommencer pagination avec nouveau token\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur inattendue : {e}\")\n",
    "            break\n",
    "    \n",
    "    return tweets_data\n",
    "\n",
    "# Utilisation\n",
    "query = \"(#beauty OR #fashion OR #tech OR #food) -is:retweet lang:en\"\n",
    "tweets = search_tweets_with_pagination(BEARER_TOKENS, query, max_results=10, max_pages=5)\n",
    "\n",
    "df_tweets = pd.DataFrame(tweets)\n",
    "print(df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1381cd56-d16c-4a1b-8457-47a5c0c42ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token chargé : None\n",
      "Erreur: 401 Unauthorized\n",
      "Unauthorized\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BEARER_TOKEN = os.getenv(\"BEARER_TOKEN_1\")\n",
    "print(\"Token chargé :\", BEARER_TOKEN)\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "try:\n",
    "    tweets = client.search_recent_tweets(query=\"hello\", max_results=5)\n",
    "    for tweet in tweets.data:\n",
    "        print(tweet.text)\n",
    "except Exception as e:\n",
    "    print(\"Erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a00f4d-c726-4801-8b63-2bee00480b3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_twitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Analyse des hashtags (sans les doublons)\u001b[39;00m\n\u001b[32m      2\u001b[39m hashtags = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df_twitter[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      4\u001b[39m     hashtags.extend(re.findall(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m#\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+\u001b[39m\u001b[33m\"\u001b[39m, text.lower()))\n\u001b[32m      6\u001b[39m Counter(hashtags).most_common(\u001b[32m10\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_twitter' is not defined"
     ]
    }
   ],
   "source": [
    "# Analyse des hashtags (sans les doublons)\n",
    "hashtags = []\n",
    "for text in df_twitter[\"text\"]:\n",
    "    hashtags.extend(re.findall(r\"#\\w+\", text.lower()))\n",
    "\n",
    "Counter(hashtags).most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e7a0d-19b2-4103-8520-822dcd439498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39c428-174d-424e-a1b2-aa5f89dda70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (Custom)",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
